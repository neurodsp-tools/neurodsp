{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using NeuroDSP with MNE\n\nThis example explores using NeuroDSP in combination with MNE.\n\nNeuroDSP is designed to offer a selection of methods and analyses that can be applied\nto neural time series. NeuroDSP itself does not offer functionality for managing multi-channel\ndata and metadata. To do so, NeuroDSP can be used with other tools, such as\n`MNE <https://mne-tools.github.io/>`_.\n\nHere, we explore an example of how analyses from NeuroDSP can be applied to data\nthat is managed and processed by MNE. In particular, it explores applying some\nmethods for finding rhythmic properties in an example electroencephalography (EEG)\ndataset, including calculating power spectra, running burst detection,\nand applying lagged coherence.\n\nThis tutorial requires that you have MNE installed. If you don't already have\nMNE, you can follow instructions to get it\n`here <https://mne-tools.github.io/stable/getting_started.html>`_.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# General imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm, colors\n\n# Import MNE, as well as the MNE sample dataset\nimport mne\nfrom mne import io\nfrom mne.datasets import sample\nfrom mne.viz import plot_topomap\n\n# Import some NeuroDSP functions to use with MNE\nfrom neurodsp.spectral import compute_spectrum, trim_spectrum\nfrom neurodsp.burst import detect_bursts_dual_threshold\nfrom neurodsp.rhythm import compute_lagged_coherence\n\n# Import NeuroDSP plotting functions\nfrom neurodsp.plts import (plot_time_series, plot_power_spectra,\n                           plot_bursts, plot_lagged_coherence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load & Check MNE Data\n\nFirst, we will load the example dataset from MNE, and have a quick look at the data.\n\nThe MNE sample dataset is a combined MEG/EEG recording with an audiovisual task, as\ndescribed `here <https://mne.tools/stable/overview/datasets_index.html?#sample>`_.\n\nFor the current example, we are going to select only the EEG data,\nand analyze it as continuous (non-epoched) data.\n\nNote that if you don't already have the data, the ``sample.data_path`` method\nwill download the MNE sample dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the data path for the MNE example data\nraw_fname = sample.data_path() + '/MEG/sample/sample_audvis_filt-0-40_raw.fif'\n\n# Load the file of example MNE data\nraw = io.read_raw_fif(raw_fname, preload=True, verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Select EEG channels from the dataset\nraw = raw.pick_types(meg=False, eeg=True, eog=False, exclude='bads')\n\n# Grab the sampling rate from the data\nfs = raw.info['sfreq']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Settings for exploring an example channel of data\nch_label = 'EEG 058'\nt_start = 20000\nt_stop = int(t_start + (10 * fs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Extract an example channel to explore\nsig, times = raw.get_data(mne.pick_channels(raw.ch_names, [ch_label]),\n                          start=t_start, stop=t_stop, return_times=True)\nsig = np.squeeze(sig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot a segment of the extracted time series data\nplot_time_series(times, sig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate Power Spectra\n\nNext lets check the data in the frequency domain, calculating a power spectrum\nwith the median Welch's procedure from NeuroDSP.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate the power spectrum, using median Welch's & extract a frequency range of interest\nfreqs, powers = compute_spectrum(sig, fs, method='welch', avg_type='median')\nfreqs, powers = trim_spectrum(freqs, powers, [3, 30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check where the peak power is\npeak_cf = freqs[np.argmax(powers)]\nprint(peak_cf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot the power spectra, and note the peak power\nplot_power_spectra(freqs, powers)\nplt.plot(freqs[np.argmax(powers)], np.max(powers), '.r', ms=12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Look for Bursts\n\nNow that we have a sense of the main rhythmicity of this piece of data, lets\nexplore using NeuroDSP to investigate whether this rhythm is bursty.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Burst settings\namp_dual_thresh = (1., 1.5)\nf_range = (peak_cf-2, peak_cf+2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Detect bursts of high amplitude oscillations in the extracted signal\nbursting = detect_bursts_dual_threshold(sig, fs, amp_dual_thresh, f_range)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot original signal and burst activity\nplot_bursts(times, sig, bursting, labels=['Raw Data', 'Detected Bursts'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure Rhythmicity with Lagged Coherence\n\nSo far, in an example channel of data, we have explored some rhythmic properties of\nthe EEG data. We did so by finding the main frequency of rhythmic power, and checking\nthis frequency for bursting.\n\nNext, we can try applying similar analyses across all channels, to measure\nrhythmic properties across the whole dataset.\n\nTo do so, let's use the lagged coherence measure. Using lagged coherence, we can measure,\nper channel, the frequency which displays the most rhythmic activity, as well as the score\nof how rhythmic this frequency is. We can do so per location, to map rhythmic activity\ntopographically across a range of frequencies.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Settings for lagged coherence, as frequency range (start, stop, step)\nf_range = (5, 25, 0.25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Calculate lagged coherence on our example channel data\nlcs, freqs = compute_lagged_coherence(sig, fs, f_range, return_spectrum=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Visualize lagged coherence across all frequencies\nplot_lagged_coherence(freqs, lcs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the most rhythmic frequency and the associated lagged coherence score\nmax_freq = freqs[np.argmax(lcs)]\nmax_score = np.max(lcs)\n\nprint('The frequency with the greatest rhythmicity is {:1.1f} Hz '\\\n      'with a lagged coherence score of {:1.2f}.'.format(max_freq, max_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set a 30 second window of data to analyze\nt_start =  10000\nt_stop = int(t_start + (60 * fs))\n\n# Initialize output variables\nmax_freq = np.zeros(len(raw.ch_names))\nmax_score = np.zeros(len(raw.ch_names))\n\n# Loop lagged coherence across each channel\nfor ind, ch_label in enumerate(raw.ch_names):\n\n    # Extract current channel data\n    cur_sig = np.squeeze(raw.get_data(mne.pick_channels(raw.ch_names, [ch_label]),\n                                      start=t_start, stop=t_stop))\n\n    # Calculate lagged coherence on current channel data\n    cur_lcs, cur_freqs = compute_lagged_coherence(cur_sig, fs, f_range, return_spectrum=True)\n\n    # Collect data of interest: frequency of max rhythmicity & associated rhythmicity value\n    max_freq[ind] = cur_freqs[np.argmax(cur_lcs)]\n    max_score[ind] = np.max(cur_lcs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Rhythmicity Across the Scalp\n\nNow that we have calculated the lagged coherence across the scalp, we can use\nsome MNE plotting utilities to plot these values across the scalp. To do so, we will\nuse the EEG montage definition of the sample dataset.\n\nIn the plots below we can see, from the plot of the lagged coherence values, that\nthe posterior electrodes display more rhythmic properties than the frontal ones.\n\nIn the next plot, of the peak frequencies, we can see that these posterior regions\nare rhythmic in the alpha range, as well as there being a central cluster of beta rhythms.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot locations with highest lagged coherence rhythmicity score\nvmin, vmax = np.min(max_score), np.max(max_score)\nplot_topomap(max_score, raw.info, cmap=cm.viridis, vmin=vmin, vmax=vmax, contours=0)\n\n# Add colorbar\nplt.figure(figsize=[2, 4])\nsm = cm.ScalarMappable(cmap='viridis', norm=colors.Normalize(vmin=vmin, vmax=vmax))\nsm.set_array(np.linspace(vmin, vmax))\ncbar = plt.colorbar(sm, orientation='vertical', label='Score')\nplt.gca().set_visible(False); plt.gcf().subplots_adjust(right=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Plot frequency with most dominant rhythmicity\nvmin, vmax = np.min(max_freq)-2, np.max(max_freq)+2\nplot_topomap(max_freq, raw.info, cmap=cm.viridis, vmin=vmin, vmax=vmax, contours=0)\n\n# Add colorbar\nplt.figure(figsize=[2, 4])\nsm = cm.ScalarMappable(cmap='viridis', norm=colors.Normalize(vmin=vmin, vmax=vmax))\nsm.set_array(np.linspace(vmin, vmax))\ncbar = plt.colorbar(sm, orientation='vertical', label='Frequency')\nplt.gca().set_visible(False); plt.gcf().subplots_adjust(right=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Analyses with MNE\n\nThat concludes the current example of using NeuroDSP in conjunction with MNE.\n\nIn this example, we applied NeuroDSP functions to continuous segments of EEG data.\n\nUsing a similar approach, you can also apply these and other analyses to epoched data,\nby managing and pre-processing data in MNE, selecting the data of interest, and then\napplying NeuroDSP functions to the data.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}